{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to SINE: SINgle Image Editing with Text-to-Image Diffusion Models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "else:\n",
    "    print(gpu_info)\n",
    "\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "    print('Not using a high-RAM runtime')\n",
    "else:\n",
    "    print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup required libraries and models. \n",
    "This may take a few minutes.\n",
    "\n",
    "You may optionally enable downloads with pydrive in order to authenticate and avoid drive download limits when fetching the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup\n",
    "\n",
    "import os\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "# install requirements\n",
    "!git clone https://github.com/zhang-zx/SINE.git sine_dir\n",
    "\n",
    "%cd sine_dir/\n",
    "!pip uninstall -y torchtext\n",
    "! pip install transformers==4.18.0 einops==0.4.1 omegaconf==2.1.1 torchmetrics==0.6.0 torch-fidelity==0.3.0 kornia==0.6 albumentations==1.1.0 opencv-python==4.2.0.34 imageio==2.14.1 setuptools==59.5.0 pillow==9.0.1 \n",
    "! pip install torch==1.10.2 torchvision==0.11.3\n",
    "! pip install pytorch-lightning==1.5.9\n",
    "! pip install git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
    "! pip install git+https://github.com/openai/CLIP.git@main#egg=clip\n",
    "! pip install -e .\n",
    "\n",
    "\n",
    "\n",
    "download_with_pydrive = True \n",
    "    \n",
    "class Downloader(object):\n",
    "    def __init__(self, use_pydrive):\n",
    "        self.use_pydrive = use_pydrive\n",
    "\n",
    "        if self.use_pydrive:\n",
    "            self.authenticate()\n",
    "        \n",
    "    def authenticate(self):\n",
    "        auth.authenticate_user()\n",
    "        gauth = GoogleAuth()\n",
    "        gauth.credentials = GoogleCredentials.get_application_default()\n",
    "        self.drive = GoogleDrive(gauth)\n",
    "    \n",
    "    def download_file(self, file_id, file_dst):\n",
    "        if self.use_pydrive:\n",
    "            downloaded = self.drive.CreateFile({'id':file_id})\n",
    "            downloaded.FetchMetadata(fetch_all=True)\n",
    "            downloaded.GetContentFile(file_dst)\n",
    "        else:\n",
    "            !gdown --id $file_id -O $file_dst\n",
    "\n",
    "downloader = Downloader(download_with_pydrive)\n",
    "\n",
    "pre_trained_path = os.path.join('models', 'ldm', 'stable-diffusion-v4')\n",
    "os.makedirs(pre_trained_path, exist_ok=True)\n",
    "!wget -O models/ldm/stable-diffusion-v4/sd-v1-4-full-ema.ckpt https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4-full-ema.ckpt \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Download the selected fine-tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_models_dir = os.path.join('./models', 'finetuned')\n",
    "os.makedirs(finetuned_models_dir, exist_ok=True)\n",
    "\n",
    "orig_image_dir = './dataset'\n",
    "os.makedirs(orig_image_dir, exist_ok=True)\n",
    "\n",
    "source_model_type = 'dog w/o patch-based fine-tuning' #@param['dog w/o patch-based fine-tuning', 'dog w/ patch-based fine-tuning', 'Girl with a peral earring', 'Monalisa', 'castle w/o patch-based fine-tuning', 'castle w/ patch-based fine-tuning']\n",
    "source_model_download_path = {\"dog w/o patch-based fine-tuning\":   \"1jHgkyxrwUXyMR2zBK9WAEWioEP3-F3fd\",\n",
    "                              \"dog w/ patch-based fine-tuning\":    \"1YI7c29qBIy83OqJ4ykoAAul6P8uaXmls\",\n",
    "                              \"Girl with a peral earring\":    \"1l6GCEfyURKQiCF77ZriYoZRtkOXQCyWD\",\n",
    "                              \"Monalisa\": \"194CDgHkomKrLvgFj89kamoTjUbMwyUIC\",\n",
    "                              \"castle w/o patch-based fine-tuning\":    \"19I8ftab9vMQWnqPH2O7aHe-GnYolmVFF\",\n",
    "                              \"castle w/ patch-based fine-tuning\":  \"1srzUr1fg6jTFKuf0M5oi5JgBsVhCt-nb\"}\n",
    "\n",
    "model_names = { \"dog w/o patch-based fine-tuning\":   \"dog_wo_patch.ckpt\",\n",
    "                \"dog w/ patch-based fine-tuning\":    \"dog_w_patch.ckpt\",\n",
    "                \"Girl with a peral earring\":    \"girl.ckpt\",\n",
    "                \"Monalisa\": \"monalisa.ckpt\",\n",
    "                \"castle w/o patch-based fine-tuning\":    \"castle_wo_patch\",\n",
    "                \"castle w/ patch-based fine-tuning\":  \"castle_w_patch\"}\n",
    "\n",
    "model_configs = { \"dog w/o patch-based fine-tuning\":   \"./configs/stable-diffusion/v1-inference.yaml\",\n",
    "                \"dog w/ patch-based fine-tuning\":    \"./configs/stable-diffusion/v1-inference_patch.yaml\",\n",
    "                \"Girl with a peral earring\":    \"./configs/stable-diffusion/v1-inference_patch_nearest.yaml\",\n",
    "                \"Monalisa\": \"./configs/stable-diffusion/v1-inference_patch_nearest.yaml\",\n",
    "                \"castle w/o patch-based fine-tuning\":    \"./configs/stable-diffusion/v1-inference.yaml\",\n",
    "                \"castle w/ patch-based fine-tuning\":  \"./configs/stable-diffusion/v1-inference_patch.yaml\"}\n",
    "\n",
    "orig_prompts = { \"dog w/o patch-based fine-tuning\":   \"picture of a sks dog\",\n",
    "                \"dog w/ patch-based fine-tuning\":    \"picture of a sks dog\",\n",
    "                \"Girl with a peral earring\":    \"painting of a sks girl\",\n",
    "                \"Monalisa\": \"painting of a sks lady\",\n",
    "                \"castle w/o patch-based fine-tuning\":    \"picture of a sks castle\",\n",
    "                \"castle w/ patch-based fine-tuning\":  \"picture of a sks castle\"}\n",
    "\n",
    "download_string = source_model_download_path[source_model_type]\n",
    "file_name = model_names[source_model_type]\n",
    "\n",
    "config_name = model_configs[source_model_type]\n",
    "fine_tune_prompt = orig_prompts[source_model_type]\n",
    "\n",
    "if not os.path.isfile(os.path.join(finetuned_models_dir, file_name)):\n",
    "    downloader.download_file(download_string, os.path.join(finetuned_models_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Edit the image with model-based guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "seed = 42\n",
    "config = OmegaConf.load('configs/stable-diffusion/v1-inference.yaml')\n",
    "model = load_model_from_config(config, 'models/ldm/stable-diffusion-v4/sd-v1-4-full-ema.ckpt')\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "sin_config = OmegaConf.load(f\"{config_name}\")\n",
    "sin_model = load_model_from_config(config, os.path.join(finetuned_models_dir, file_name))\n",
    "sin_model = sin_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "K_min = 400 #@param {type:\"slider\", min:0, max:1000, step:10}\n",
    "scale = 7.5 #@param {type:\"slider\", min:1.0, max:50, step:0.5}\n",
    "ddim_steps = 100\n",
    "ddim_eta = 0.\n",
    "H = 512\n",
    "W = 512\n",
    "\n",
    "prompt = \"a dog wearing a superhero cape\" #@param {'type': 'string'}\n",
    "\n",
    "extra_config = {\n",
    "    'cond_beta': v,\n",
    "    'cond_beta_sin': 1. - v,\n",
    "    'range_t_max': 1000,\n",
    "    'range_t_min': K_min\n",
    "}\n",
    "\n",
    "\n",
    "from ldm.models.diffusion.guidance_ddim import DDIMSinSampler\n",
    "sampler = DDIMSinSampler(model, sin_model)\n",
    "\n",
    "setattr(sampler.model, 'extra_config', extra_config)\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "n_rows = 2\n",
    "start_code = None\n",
    "precision_scope = autocast\n",
    "num_samples = 4\n",
    "\n",
    "all_samples = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            with sin_model.ema_scope():\n",
    "                tic = time.time()\n",
    "                all_samples = list()\n",
    "                for n in trange(num_samples, desc=\"Sampling\"):   \n",
    "                    uc = None\n",
    "                    if scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                        uc_sin = sin_model.get_learned_conditioning(batch_size * [\"\"])\n",
    "\n",
    "                    prompts = [prompt] * batch_size\n",
    "                    prompts_single = [fine_tune_prompt] * batch_size\n",
    "                    \n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                    c_sin = sin_model.get_learned_conditioning(prompts_single)\n",
    "                    \n",
    "                    shape = [4, H // 8, W // 8]\n",
    "                    samples_ddim, _ = sampler.sample( S=ddim_steps,\n",
    "                                                      conditioning=c,\n",
    "                                                      conditioning_single=c_sin,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shape=shape,\n",
    "                                                      verbose=False,\n",
    "                                                      unconditional_guidance_scale=scale,\n",
    "                                                      unconditional_conditioning=uc,\n",
    "                                                      unconditional_conditioning_single=uc_sin,\n",
    "                                                      eta=ddim_eta,\n",
    "                                                      x_T=start_code)\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                    all_samples.append(x_samples_ddim)\n",
    "\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                \n",
    "                grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                # to image\n",
    "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                os.makedirs('./output', exist_ok=True)\n",
    "                Image.fromarray(grid.astype(np.uint8)).save(os.path.join('./output', f'{prompt.replace(\" \", \"-\")}.jpg'))\n",
    "                display(Image.open(os.path.join('./output', f'{prompt.replace(\" \", \"-\")}.jpg')))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('SINE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a84f578b9cda1db545aa6690161d7775d6ea32a647f25bb9ef4866c136688289"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
